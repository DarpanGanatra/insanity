{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "Linear regression is a pretty simple topic, I'm not going to go into too much detail into it. So here's how you're going to use Sklearn and then statsmodels to do linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "There are a bunch of things I'm not doing here, which you should do before you start modeling. Some examples: \n",
    "\n",
    "- Checking for missing values\n",
    "- Checking for outliers\n",
    "- Checking for multicollinearity\n",
    "- Scaling the data\n",
    "\n",
    "Keep this in mind before you start modeling on real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "X, y = load_diabetes(return_X_y=True, as_frame=True)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit model\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the model, we can check the coefficients and the intercept to see what values we got fitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age: 7.451235254802735\n",
      "sex: -244.21964310531743\n",
      "bmi: 550.7658427308809\n",
      "bp: 347.90680714274094\n",
      "s1: -944.2082992346313\n",
      "s2: 634.3733817078077\n",
      "s3: 179.0443878860314\n",
      "s4: 172.67186895708974\n",
      "s5: 839.4036942542416\n",
      "s6: 28.58249143883081\n",
      "Intercept: 155.46753883306948\n"
     ]
    }
   ],
   "source": [
    "for feat, coef in zip(X.columns, model.coef_):\n",
    "    print(f\"{feat}: {coef}\")\n",
    "print(f\"Intercept: {model.intercept_}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's our RMSE and R^2 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 55.80718564102841\n",
      "R^2: 0.369279653582653\n"
     ]
    }
   ],
   "source": [
    "rmse = mean_squared_error(y_test, model.predict(X_test), squared=False)\n",
    "print(f\"RMSE: {rmse}\")\n",
    "\n",
    "r2 = model.score(X_test, y_test)\n",
    "print(f\"R^2: {r2}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_model = sm.OLS(y_train, sm.add_constant(X_train)).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>target</td>      <th>  R-squared:         </th> <td>   0.542</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.529</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   40.47</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 17 May 2023</td> <th>  Prob (F-statistic):</th> <td>3.37e-52</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>14:02:16</td>     <th>  Log-Likelihood:    </th> <td> -1903.3</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   353</td>      <th>  AIC:               </th> <td>   3829.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   342</td>      <th>  BIC:               </th> <td>   3871.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    10</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>  155.4675</td> <td>    2.879</td> <td>   54.000</td> <td> 0.000</td> <td>  149.805</td> <td>  161.130</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>age</th>   <td>    7.4512</td> <td>   65.686</td> <td>    0.113</td> <td> 0.910</td> <td> -121.748</td> <td>  136.651</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sex</th>   <td> -244.2196</td> <td>   69.706</td> <td>   -3.504</td> <td> 0.001</td> <td> -381.326</td> <td> -107.113</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bmi</th>   <td>  550.7658</td> <td>   72.986</td> <td>    7.546</td> <td> 0.000</td> <td>  407.208</td> <td>  694.324</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>bp</th>    <td>  347.9068</td> <td>   73.380</td> <td>    4.741</td> <td> 0.000</td> <td>  203.573</td> <td>  492.241</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>s1</th>    <td> -944.2083</td> <td>  483.310</td> <td>   -1.954</td> <td> 0.052</td> <td>-1894.842</td> <td>    6.425</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>s2</th>    <td>  634.3734</td> <td>  392.838</td> <td>    1.615</td> <td> 0.107</td> <td> -138.310</td> <td> 1407.057</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>s3</th>    <td>  179.0444</td> <td>  250.939</td> <td>    0.713</td> <td> 0.476</td> <td> -314.533</td> <td>  672.622</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>s4</th>    <td>  172.6719</td> <td>  184.352</td> <td>    0.937</td> <td> 0.350</td> <td> -189.936</td> <td>  535.279</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>s5</th>    <td>  839.4037</td> <td>  197.026</td> <td>    4.260</td> <td> 0.000</td> <td>  451.868</td> <td> 1226.939</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>s6</th>    <td>   28.5825</td> <td>   73.086</td> <td>    0.391</td> <td> 0.696</td> <td> -115.172</td> <td>  172.337</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 1.688</td> <th>  Durbin-Watson:     </th> <td>   1.971</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.430</td> <th>  Jarque-Bera (JB):  </th> <td>   1.630</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.087</td> <th>  Prob(JB):          </th> <td>   0.443</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.716</td> <th>  Cond. No.          </th> <td>    237.</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &      target      & \\textbf{  R-squared:         } &     0.542   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.529   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     40.47   \\\\\n",
       "\\textbf{Date:}             & Wed, 17 May 2023 & \\textbf{  Prob (F-statistic):} &  3.37e-52   \\\\\n",
       "\\textbf{Time:}             &     14:02:16     & \\textbf{  Log-Likelihood:    } &   -1903.3   \\\\\n",
       "\\textbf{No. Observations:} &         353      & \\textbf{  AIC:               } &     3829.   \\\\\n",
       "\\textbf{Df Residuals:}     &         342      & \\textbf{  BIC:               } &     3871.   \\\\\n",
       "\\textbf{Df Model:}         &          10      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "               & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{const} &     155.4675  &        2.879     &    54.000  &         0.000        &      149.805    &      161.130     \\\\\n",
       "\\textbf{age}   &       7.4512  &       65.686     &     0.113  &         0.910        &     -121.748    &      136.651     \\\\\n",
       "\\textbf{sex}   &    -244.2196  &       69.706     &    -3.504  &         0.001        &     -381.326    &     -107.113     \\\\\n",
       "\\textbf{bmi}   &     550.7658  &       72.986     &     7.546  &         0.000        &      407.208    &      694.324     \\\\\n",
       "\\textbf{bp}    &     347.9068  &       73.380     &     4.741  &         0.000        &      203.573    &      492.241     \\\\\n",
       "\\textbf{s1}    &    -944.2083  &      483.310     &    -1.954  &         0.052        &    -1894.842    &        6.425     \\\\\n",
       "\\textbf{s2}    &     634.3734  &      392.838     &     1.615  &         0.107        &     -138.310    &     1407.057     \\\\\n",
       "\\textbf{s3}    &     179.0444  &      250.939     &     0.713  &         0.476        &     -314.533    &      672.622     \\\\\n",
       "\\textbf{s4}    &     172.6719  &      184.352     &     0.937  &         0.350        &     -189.936    &      535.279     \\\\\n",
       "\\textbf{s5}    &     839.4037  &      197.026     &     4.260  &         0.000        &      451.868    &     1226.939     \\\\\n",
       "\\textbf{s6}    &      28.5825  &       73.086     &     0.391  &         0.696        &     -115.172    &      172.337     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       &  1.688 & \\textbf{  Durbin-Watson:     } &    1.971  \\\\\n",
       "\\textbf{Prob(Omnibus):} &  0.430 & \\textbf{  Jarque-Bera (JB):  } &    1.630  \\\\\n",
       "\\textbf{Skew:}          & -0.087 & \\textbf{  Prob(JB):          } &    0.443  \\\\\n",
       "\\textbf{Kurtosis:}      &  2.716 & \\textbf{  Cond. No.          } &     237.  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                 target   R-squared:                       0.542\n",
       "Model:                            OLS   Adj. R-squared:                  0.529\n",
       "Method:                 Least Squares   F-statistic:                     40.47\n",
       "Date:                Wed, 17 May 2023   Prob (F-statistic):           3.37e-52\n",
       "Time:                        14:02:16   Log-Likelihood:                -1903.3\n",
       "No. Observations:                 353   AIC:                             3829.\n",
       "Df Residuals:                     342   BIC:                             3871.\n",
       "Df Model:                          10                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const        155.4675      2.879     54.000      0.000     149.805     161.130\n",
       "age            7.4512     65.686      0.113      0.910    -121.748     136.651\n",
       "sex         -244.2196     69.706     -3.504      0.001    -381.326    -107.113\n",
       "bmi          550.7658     72.986      7.546      0.000     407.208     694.324\n",
       "bp           347.9068     73.380      4.741      0.000     203.573     492.241\n",
       "s1          -944.2083    483.310     -1.954      0.052   -1894.842       6.425\n",
       "s2           634.3734    392.838      1.615      0.107    -138.310    1407.057\n",
       "s3           179.0444    250.939      0.713      0.476    -314.533     672.622\n",
       "s4           172.6719    184.352      0.937      0.350    -189.936     535.279\n",
       "s5           839.4037    197.026      4.260      0.000     451.868    1226.939\n",
       "s6            28.5825     73.086      0.391      0.696    -115.172     172.337\n",
       "==============================================================================\n",
       "Omnibus:                        1.688   Durbin-Watson:                   1.971\n",
       "Prob(Omnibus):                  0.430   Jarque-Bera (JB):                1.630\n",
       "Skew:                          -0.087   Prob(JB):                        0.443\n",
       "Kurtosis:                       2.716   Cond. No.                         237.\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our coefficients and intercept are the same as the ones we got from Sklearn. Note that I had to add a constant to the data before fitting the model. This is because statsmodels doesn't add a constant by default, so keep that in mind.\n",
    "\n",
    "Now taking a look at the RMSE and R^2 values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 55.80718564102823\n",
      "R^2: 0.5420114235281142\n"
     ]
    }
   ],
   "source": [
    "rmse = mean_squared_error(y_test, sm_model.predict(sm.add_constant(X_test)), squared=False)\n",
    "print(f\"RMSE: {rmse}\")\n",
    "\n",
    "r2 = sm_model.rsquared\n",
    "print(f\"R^2: {r2}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, they're the same. Why is that expected? Because we have the same coefficients. At the end of the day, we've found the same line (hyperplane) of best fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
